import os
from pathlib import Path

import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI

from scripts.retrieve_tfidf import search


load_dotenv()

MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")


def get_api_key():
    # Streamlit secrets Ã¶ncelikli, yoksa env
    if "OPENAI_API_KEY" in st.secrets:
        return st.secrets["OPENAI_API_KEY"]
    return os.getenv("OPENAI_API_KEY", "")


def ensure_index():
    if not Path("index/tfidf_vectorizer.pkl").exists():
        from scripts.build_index_tfidf import main as build_main

        build_main()


SYSTEM_PROMPT = """Sen Ä°stanbul Ãœniversitesi Ä°ÅŸletme FakÃ¼ltesi iÃ§in yÃ¶netmelik/SSS gibi metinlerden yanÄ±t veren bir asistansÄ±n.
Kurallar:
- YanÄ±t dili TÃ¼rkÃ§e.
- Sadece verilen kaynak parÃ§alarÄ±na dayan.
- EÄŸer kaynak parÃ§alarÄ±nda net cevap yoksa "Bu belgelerde net bilgi yok" de ve hangi belgenin hangi sayfasÄ±na baktÄ±ÄŸÄ±nÄ± belirt.
- En sonda kaynaklarÄ± madde madde yaz: (dosya | sayfa).
"""


def format_context(hits):
    parts = []
    for h in hits:
        parts.append(f"[Kaynak: {h['source']} | sayfa {h['page']} | skor {h['score']:.3f}]")
        parts.append(h["text"])
        parts.append("")
    return "\n".join(parts).strip()


st.set_page_config(page_title="Ä°Ãœ Ä°ÅŸletme Bot", page_icon="ğŸ“", layout="centered")
st.title("ğŸ“ Ä°Ãœ Ä°ÅŸletme Bot")
st.caption("TF-IDF ile dokÃ¼man bulur, GPT ile cevaplar. Kaynak gÃ¶sterir.")

ensure_index()

api_key = get_api_key()
if not api_key:
    st.error("OPENAI_API_KEY bulunamadÄ±. Lokal iÃ§in .env, Streamlit Cloud iÃ§in Secrets ekle.")
    st.stop()

client = OpenAI(api_key=api_key)

if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "Merhaba. Sorunu yaz, ilgili belge parÃ§alarÄ±nÄ± bulup kaynaklÄ± cevap vereyim."}
    ]


for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])


q = st.chat_input("Soru yaz (Ã¶r: Mazeret sÄ±navÄ± hangi maddeye gÃ¶re yapÄ±lÄ±r?)")
if q:
    st.session_state.messages.append({"role": "user", "content": q})
    with st.chat_message("user"):
        st.markdown(q)

    hits = search(q, top_k=5)
    context = format_context(hits)

    user_prompt = f"""Soru: {q}

AÅŸaÄŸÄ±daki kaynak parÃ§alarÄ±nÄ± kullanarak cevap ver:
{context}
"""

    with st.chat_message("assistant"):
        out = st.empty()
        full = ""

        try:
            stream = client.chat.completions.create(
                model=MODEL,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=0.2,
                max_tokens=350,
                stream=True,
            )

            for evt in stream:
                delta = evt.choices[0].delta.content or ""
                if delta:
                    full += delta
                    out.markdown(full)

        except Exception as e:
            # Quota / 429 vs. her ÅŸey iÃ§in: fallback
            full = "âŒ GPT Ã§aÄŸrÄ±sÄ± baÅŸarÄ±sÄ±z oldu. Åimdilik sadece dokÃ¼man parÃ§alarÄ±nÄ± gÃ¶sterebiliyorum.\n\n"
            full += f"Hata: `{str(e)}`\n\n"
            full += "### Bulunan parÃ§alar\n"
            for h in hits:
                full += f"- **{h['source']}**, sayfa **{h['page']}** (skor {h['score']:.3f})\n"
            out.markdown(full)

    st.session_state.messages.append({"role": "assistant", "content": full})